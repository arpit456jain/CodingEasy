{% extends 'home/base.html' %}
{% load static %}

{% block content %}

<style>


@media (min-width: 768px) {
.main {
padding-right: 40px;
padding-left: 220px; /* 180 + 40 */
}
}


.sidebar {
position: fixed;
top: 75px;
bottom: 0;
left: 0;
z-index:1;
width: 290px;
display: block;
padding: 20px;
overflow-x: hidden;
overflow-y: auto; /* Scrollable contents if viewport is shorter than content. */
background-color:blue;
font-color:white;
border-right: 1px solid #eee;
}
</style>



<div class="container-sm">
<div class="row">

<div class="sidebar">


  <div class="col-sm-3 col-md-2 sidebar">



      <h2 ><a href="ML.html" class="btn btn-primary btn-lg btn-block" style="color:white;">Introduction</a></h2><br />
      <h2 ><a href="{% url 'ml1' %}?data=linregx" class="btn btn-primary btn-lg btn-block" style="color:white;">Linear Regression</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=logregx" style="color:white;">Logistic Regression</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=knn" class="btn btn-primary btn-lg btn-block" style="color:white;">k-nearest neighbours</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=svm" class="btn btn-primary btn-lg btn-block" style="color:white;">Support Vector machines</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=decitree" class="btn btn-primary btn-lg btn-block" style="color:white;">Decision Tree</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=rf" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Random Forest</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=nv" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Naive Bayes</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=kmc" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">k Means Clustering</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=hrc" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Hierarchical Clustering</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=ohe" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">One Hot Encoding</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=kcv" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">K-Fold Cross Validation</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=gs" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Grid Search</a></h2><br/>





</div>
</div>
</div>
</div>

<section class="container mt-3">
    <div class="row">
        <div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
            <div class="jumbotron px-3 py-4 px-sm-4 py-sm-5 bg-light rounded-3 mb-3">
                <h1 class="text-center">One Hot Encoding</h1>
                <p></p>
                <ul>
                    <li>One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.</li>
                </ul>

            </div>
        </div>
    </div>

</section>
<br><br>

<section class="container mt-5">
    <div class="row">
        <div class="col-md-6 col-sm-12 main">
            <h3 class="text-center">One Hot Encoding</h3>
            <p>It is a stage that is performed in Data pre-processing. As we know that most of the Machine Learning 
                algorithms cannot perform on label data directly. They require all the input and output variables 
                to be numeric.</p>
            <p>One such way of converting label data to numeric data is One Hot Encoding.
                In case of the variables that are categorical with the absence of ordinal relationaship, interger 
                encoding is not enough. Using the encoding, thus making the model assume a natural ordering between 
                the processes can end up in poor performance or unexpected results.</p>
            <p>Hence, one-hot encoding can be applied for integer representation. The encoded variable is removed 
                and a new binary variable gets assigned for each new integer value. The binary variables, thus created, 
                are also called dummy variables.</p>
            <p>One Hot Encoding can often lead to dummy variable trap. It occurs when occurs when two or more dummy 
                variables created by one-hot encoding are highly correlated. Whenever one variable can be derived 
                from rest if the variables, these variables are said to be multi co-linear. When there is multi 
                co-linearity in dataset, the problem of dummy variable trap arises which can mess up our model. So the 
                simple rule is that we need to drop one of the dummy variables column from our dataset.
            </p>

        </div>
        <div class="col-md-6 col-sm-12">
            <h3 class="text-center">Example</h3>
           <br>
                    <img src="{% static 'img/onehot.png' %}",height=600px, width=600px>
            
           
        </div>
    </div>
</section>
<br><br>


{% endblock content %}
