{% extends 'home/base.html' %}
{% load static %}

{% block content %}

<style>


@media (min-width: 768px) {
.main {
padding-right: 40px;
padding-left: 220px; /* 180 + 40 */
}
}


.sidebar {
position: fixed;
top: 75px;
bottom: 0;
left: 0;
z-index:1;
width: 290px;
display: block;
padding: 20px;
overflow-x: hidden;
overflow-y: auto; /* Scrollable contents if viewport is shorter than content. */
background-color:blue;
font-color:white;
border-right: 1px solid #eee;
}
</style>



<div class="container-sm">
<div class="row">

<div class="sidebar">


  <div class="col-sm-3 col-md-2 sidebar">



      <h2 ><a href="ML.html" class="btn btn-primary btn-lg btn-block" style="color:white;">Introduction</a></h2><br />
      <h2 ><a href="{% url 'ml1' %}?data=linregx" class="btn btn-primary btn-lg btn-block" style="color:white;">Linear Regression</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=logregx" style="color:white;">Logistic Regression</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=knn" class="btn btn-primary btn-lg btn-block" style="color:white;">k-nearest neighbours</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=svm" class="btn btn-primary btn-lg btn-block" style="color:white;">Support Vector machines</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=decitree" class="btn btn-primary btn-lg btn-block" style="color:white;">Decision Tree</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=rf" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Random Forest</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=nv" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Naive Bayes</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=kmc" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">k Means Clustering</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=hrc" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Hierarchical Clustering</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=ohe" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">One Hot Encoding</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=kcv" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">K-Fold Cross Validation</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=gs" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Grid Search</a></h2><br/>





</div>
</div>
</div>
</div>

<section class="container mt-3">
    <div class="row">
        <div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
            <div class="jumbotron px-3 py-4 px-sm-4 py-sm-5 bg-light rounded-3 mb-3">
                <h1 class="text-center">K-Fold Cross Validation</h1>
                <p><br></p>
                <ul>
                    <li>Cross-validation is a technique for evaluating a machine learning model and testing its performance. </li>
                    <li>K-Fold cross-validation is a technique that minimizes the disadvantages of the hold-out method. K-Fold introduces a new way of splitting the dataset which helps to overcome the “test only once bottleneck”.</li>
                </ul>

            </div>
        </div>
    </div>

</section>
<br><br>

<section class="container mt-5">
    <div class="row">
        <div class="col-md-6 col-sm-12 main">
            <h3 class="text-center">K-Fold Cross Validation</h3>
            <p>Cross validation is a statistical method that helps in evaluating the performance of the 
                model. For example, if we are creating ML models for classifying whether emails are spam or not.</p>
            <p>In order to train a model in such a way, we use whatever label data we have and once the model is built, 
                we test it using different datassets. Model will return us different reult when we compare with the 
                truth to measure accuracy.</p>
            <p>The k value in K-Fold Cross Validation refers to the number of groups that are to be formed. This is
                done by splitting the data sample into 'k' number of groups. Hence, the k value must be chosen 
                carefully for the data sample.</p>
            <p>The model training can be done in the following ways:
                1. Use all the available data for training and test the model on the same dataset. Here
                   we are using 100% of the samples for testing the model as well as using the same 
                   exit samples to test the model.
                2. Split the available datasets into training and testing sets.
                3. K-Fold Cross Validation, where we divide our samples into folds. After that we run multiple
                   iterations. For example, we can use the first fold for testing the model, rest for training.
                   Then we can use only the second fold for testing, other for training. The process can be 
                   repeated till the last fold. After that, we have the scores of each and average them out. 
            <p>A basic example is demonstrated below:
            from sklearn.model_selection import KFold #using k_fold api
            kf = KFold(n_splits=3) #n_splits=3 is used to give how many fols you want to create, here it is 3
            kf</p>
            <p>sklearn has cross_val_score to test the scores of different models.
                Command:
                from sklearn.model_selection import cross_val_score
            </p>

        </div>
        <div class="col-md-6 col-sm-12">
            <h3 class="text-center">Example</h3>
            <br>
                    <img src="{% static 'img/kfold.jpeg' %}",height=600px, width=600px>

        </div>
    </div>
</section>
<br><br>


{% endblock content %}
