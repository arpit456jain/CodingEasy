{% extends 'home/base.html' %}
{% load static %}

{% block content %}

<style>


@media (min-width: 768px) {
.main {
padding-right: 40px;
padding-left: 220px; /* 180 + 40 */
}
}


.sidebar {
position: fixed;
top: 75px;
bottom: 0;
left: 0;
z-index:1;
width: 290px;
display: block;
padding: 20px;
overflow-x: hidden;
overflow-y: auto; /* Scrollable contents if viewport is shorter than content. */
background-color:blue;
font-color:white;
border-right: 1px solid #eee;
}
</style>



<div class="container-sm">
<div class="row">

<div class="sidebar">


  <div class="col-sm-3 col-md-2 sidebar">



      <h2 ><a href="ML.html" class="btn btn-primary btn-lg btn-block" style="color:white;">Introduction</a></h2><br />
      <h2 ><a href="{% url 'ml1' %}?data=linregx" class="btn btn-primary btn-lg btn-block" style="color:white;">Linear Regression</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=logregx" style="color:white;">Logistic Regression</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=knn" class="btn btn-primary btn-lg btn-block" style="color:white;">k-nearest neighbours</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=svm" class="btn btn-primary btn-lg btn-block" style="color:white;">Support Vector machines</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=decitree" class="btn btn-primary btn-lg btn-block" style="color:white;">Decision Tree</a></h2><br />
      <h2><a href="{% url 'ml1' %}?data=rf" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Random Forest</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=nv" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Naive Bayes</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=kmc" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">k Means Clustering</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=hrc" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Hierarchical Clustering</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=ohe" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">One Hot Encoding</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=kcv" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">K-Fold Cross Validation</a></h2><br/>
      <h2><a href="{% url 'ml1' %}?data=gs" class="btn btn-primary btn-lg btn-block" style="color:white;text-align:center;">Grid Search</a></h2><br/>





</div>
</div>
</div>
</div>

<section class="container mt-3">
    <div class="row">
        <div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">
            <div class="jumbotron px-3 py-4 px-sm-4 py-sm-5 bg-light rounded-3 mb-3">
                <h1 class="text-center">Support Vector Machines (Regression & Classification Algorithm)</h1>

            </div>
</section>
<br><br>

<section class="container mt-5">
    <div class="row">
        <div class="col-md-6 col-sm-12 main">
            <h3 class="text-center">Support Vector Machines</h3>
            <p>Support Vector Machines is a Regression & Classification Algorithm. When used for Regression it's called
                Support Vector Regressor(SVR) & when it's used for Classification it's called Support Vector
                Classifier(SVC)</p>
            <p>Logistic Regression fails when it comes to a dataset with complex features and in those cases of binary
                classification SVM plays a major role to give more accurate results </p>
            <p>A Sharp Line Separates the classes. The orange line is the sharp separating line also called
                Hyperplane/Decision Boundary. Here we have 2 classes - class 0(blue/star markers), class 1(green/circle
                markers)</p>
            <p>We calculate the distance of nearest points of both the classes from the hyperplane. The nearest points
                are encircled with red and are called Support Vectors/Extreme Points. Now suppose their distances are d1
                and d2 respectively so we
                find a total distance d=d1+d2 which is called Width Of The Decision Boundary. Now we try to minimize
                this distance to find the best fit line as Hyperplane such that the width of decision boundary is
                minimum.
            </p>
        </div>
        <div class="col-md-6 col-sm-12">
            <h3 class="text-center">Plot Picture</h3>
            <div class="code">
                <pre>
                    <img src="{% static 'img/VectorMachinesScatter_Plot.jpg' %}",height=500px, width=500px>
        </pre>
            </div>
        </div>
    </div>
</section>
<br><br>


<br><br>

{% endblock content %}
